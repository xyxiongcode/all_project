#!/usr/bin/env python3
"""
CUDAå†…å­˜æ£€æŸ¥å’Œç®¡ç†å·¥å…·
ç”¨äºæŸ¥çœ‹GPUå†…å­˜ä½¿ç”¨æƒ…å†µå’Œæ¸…ç†å†…å­˜
"""
import torch
import subprocess
import sys


def print_memory_info():
    """æ‰“å°è¯¦ç»†çš„CUDAå†…å­˜ä¿¡æ¯"""
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    print("=" * 60)
    print("CUDA å†…å­˜ä¿¡æ¯")
    print("=" * 60)
    
    # PyTorchå†…å­˜ä¿¡æ¯
    device = torch.device('cuda:0')
    allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB
    reserved = torch.cuda.memory_reserved(device) / 1024**3  # GB
    max_allocated = torch.cuda.max_memory_allocated(device) / 1024**3  # GB
    max_reserved = torch.cuda.max_memory_reserved(device) / 1024**3  # GB
    
    print(f"\nğŸ“Š PyTorchå†…å­˜ç»Ÿè®¡ (GPU 0):")
    print(f"  å·²åˆ†é…å†…å­˜: {allocated:.2f} GB")
    print(f"  é¢„ç•™å†…å­˜: {reserved:.2f} GB")
    print(f"  æœ€å¤§å·²åˆ†é…: {max_allocated:.2f} GB")
    print(f"  æœ€å¤§é¢„ç•™: {max_reserved:.2f} GB")
    
    # nvidia-smiä¿¡æ¯
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.total,memory.used,memory.free', 
                                '--format=csv,noheader,nounits'], 
                               capture_output=True, text=True, check=True)
        print(f"\nğŸ“ˆ nvidia-smi ä¿¡æ¯:")
        lines = result.stdout.strip().split('\n')
        for line in lines:
            parts = line.split(', ')
            if len(parts) >= 5:
                idx, name, total, used, free = parts[0], parts[1], parts[2], parts[3], parts[4]
                total_gb = float(total) / 1024
                used_gb = float(used) / 1024
                free_gb = float(free) / 1024
                print(f"  GPU {idx} ({name}):")
                print(f"    æ€»å†…å­˜: {total_gb:.2f} GB")
                print(f"    å·²ä½¿ç”¨: {used_gb:.2f} GB ({used_gb/total_gb*100:.1f}%)")
                print(f"    ç©ºé—²: {free_gb:.2f} GB ({free_gb/total_gb*100:.1f}%)")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹
                result_proc = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,process_name,used_memory', 
                                             '--format=csv,noheader,nounits'], 
                                            capture_output=True, text=True, check=True)
                if result_proc.stdout.strip():
                    print(f"\n  æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹:")
                    for proc_line in result_proc.stdout.strip().split('\n'):
                        proc_parts = proc_line.split(', ')
                        if len(proc_parts) >= 3:
                            pid, name, mem = proc_parts[0], proc_parts[1], proc_parts[2]
                            mem_gb = float(mem) / 1024
                            print(f"    PID {pid}: {name} ({mem_gb:.2f} GB)")
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸  æ— æ³•è·å–nvidia-smiä¿¡æ¯: {e}")
    except FileNotFoundError:
        print("âš ï¸  nvidia-smiå‘½ä»¤ä¸å¯ç”¨")
    
    print("=" * 60)


def clear_cache():
    """æ¸…ç†CUDAç¼“å­˜"""
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†ç¼“å­˜")
        return
    
    print("\nğŸ§¹ æ¸…ç†CUDAç¼“å­˜...")
    
    # æ¸…ç©ºç¼“å­˜
    torch.cuda.empty_cache()
    
    # é‡ç½®å³°å€¼ç»Ÿè®¡
    torch.cuda.reset_peak_memory_stats()
    
    # åŒæ­¥
    torch.cuda.synchronize()
    
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    
    print(f"âœ… ç¼“å­˜å·²æ¸…ç†")
    print(f"   å½“å‰å·²åˆ†é…: {allocated:.2f} GB")
    print(f"   å½“å‰é¢„ç•™: {reserved:.2f} GB")


def kill_processes_by_name(process_names):
    """æ ¹æ®è¿›ç¨‹åæ€æ­»è¿›ç¨‹"""
    import psutil
    killed = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        try:
            cmdline = ' '.join(proc.info['cmdline'] or [])
            for name in process_names:
                if name in cmdline:
                    proc.kill()
                    killed.append((proc.info['pid'], proc.info['name']))
                    print(f"âœ… å·²ç»ˆæ­¢è¿›ç¨‹: PID {proc.info['pid']} ({proc.info['name']})")
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass
    return killed


if __name__ == "__main__":
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "info" or command == "i":
            print_memory_info()
        elif command == "clear" or command == "c":
            clear_cache()
            print_memory_info()
        elif command == "kill":
            print("ğŸ” æŸ¥æ‰¾Pythonè®­ç»ƒè¿›ç¨‹...")
            killed = kill_processes_by_name(['train.py', 'navdp_train.py', 'python'])
            if not killed:
                print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¿›ç¨‹")
        elif command == "all" or command == "a":
            print("ğŸ” æŸ¥æ‰¾å¹¶ç»ˆæ­¢ç›¸å…³è¿›ç¨‹...")
            kill_processes_by_name(['train.py', 'navdp_train.py'])
            print("\nğŸ§¹ æ¸…ç†ç¼“å­˜...")
            clear_cache()
            print("\nğŸ“Š å½“å‰å†…å­˜çŠ¶æ€:")
            print_memory_info()
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
            print("\nç”¨æ³•:")
            print("  python check_cuda_memory.py info   - æŸ¥çœ‹å†…å­˜ä¿¡æ¯")
            print("  python check_cuda_memory.py clear  - æ¸…ç†ç¼“å­˜")
            print("  python check_cuda_memory.py kill   - ç»ˆæ­¢ç›¸å…³è¿›ç¨‹")
            print("  python check_cuda_memory.py all    - æ‰§è¡Œæ‰€æœ‰æ“ä½œ")
    else:
        print_memory_info()

